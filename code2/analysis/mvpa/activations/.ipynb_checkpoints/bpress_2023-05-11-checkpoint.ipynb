{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "willing-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python resample.py $IMAGE_TO_RESAMPLE $REFERENCE_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afraid-clause",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook bpress_2023-05-11.ipynb to python\n",
      "[NbConvertApp] Writing 20666 bytes to bpress_2023-05-11.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert bpress_2023-05-11.ipynb --to python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "spiritual-oasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n~~~ Script description ~~~\\nadapted from troubshoot_bpress_slopes_6-24-22\\nadapted from ATM_bpress_within-between_1-4-23_rest.py on 1/31/23\\n- changed bpress to exclude overlap\\nchanged path, added sys.argv for multiple jobs 5/11/23\\n- roi_activations_2023-05-11.npy is the same dictionary as\\n  mvpa_results_2023-05-11.npy with more windows and better name  \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "~~~ Script description ~~~\n",
    "adapted from troubshoot_bpress_slopes_6-24-22\n",
    "adapted from ATM_bpress_within-between_1-4-23_rest.py on 1/31/23\n",
    "- changed bpress to exclude overlap\n",
    "changed path, added sys.argv for multiple jobs 5/11/23\n",
    "- roi_activations_2023-05-11.npy is the same dictionary as\n",
    "  mvpa_results_2023-05-11.npy with more windows and better name  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "meaning-enclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nilearn/datasets/__init__.py:90: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "from nilearn.input_data import NiftiMasker , MultiNiftiMasker\n",
    "\n",
    "import nilearn as nil\n",
    "\"\"\"\n",
    "Plotting!\n",
    "https://nilearn.github.io/plotting/index.html\n",
    "\n",
    "\"\"\"\n",
    "from nilearn import plotting\n",
    "import numpy as np \n",
    "import os\n",
    "import os.path\n",
    "import scipy.io\n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.masking import compute_epi_mask\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from copy import deepcopy\n",
    "\n",
    "import warnings\n",
    "import sys  \n",
    "import random\n",
    "# import logging\n",
    "\n",
    "import deepdish as dd\n",
    "import numpy as np\n",
    "\n",
    "import brainiak.eventseg.event\n",
    "import nibabel as nib\n",
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, zscore, pearsonr\n",
    "from scipy.signal import gaussian, convolve\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import LeaveOneOut, KFold\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns \n",
    "\n",
    "sns.set(style = 'white', context='talk', font_scale=1, rc={\"lines.linewidth\": 2})\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\"\"\"\n",
    "from utils import sherlock_h5_data\n",
    "\n",
    "if not os.path.exists(sherlock_h5_data):\n",
    "    os.makedirs(sherlock_h5_data)\n",
    "    print('Make dir: ', sherlock_h5_data)\n",
    "else: \n",
    "    print('Data path exists')\n",
    "    \n",
    "from utils import sherlock_dir\n",
    "\"\"\"\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "from brainiak import image, io\n",
    "from scipy.stats import stats\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from brainiak import image, io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "\n",
    "import pandas as pd\n",
    "from nilearn.plotting import plot_glass_brain\n",
    "\n",
    "# Import machine learning libraries\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold, f_classif, SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import sem\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import statistics\n",
    "\n",
    "from nilearn import datasets, plotting\n",
    "from nilearn.input_data import NiftiSpheresMasker\n",
    "\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from nilearn.image import concat_imgs, resample_img, mean_img, index_img\n",
    "from nilearn import image\n",
    "from nilearn import masking\n",
    "from nilearn.plotting import view_img\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "from nilearn.reporting import get_clusters_table\n",
    "from nilearn import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-armor",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "measured-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expand / Label TRs\n",
    "\"\"\"\n",
    "0 = SM\n",
    "1 = SC\n",
    "2 = OM\n",
    "3 = OC\n",
    "4 = Re\n",
    "requires list of labels ouputed by psychopy (column 1 - MM_self_title.started, etc.)\n",
    "returns label list (order is preserved) and TR labels\n",
    "\"\"\"\n",
    "\n",
    "def label_lists(label, num_tr):\n",
    "    b = [[]]\n",
    "    a = []\n",
    "    for i in label:\n",
    "        # substring label in psychopy output\n",
    "        # if the first three characters == M_s, etc, then add correct indext to string\n",
    "        if i[1:4] == \"M_s\":\n",
    "            a.append(\"SM\")\n",
    "            b.append([0]*num_tr)\n",
    "        elif i[1:4] == \"C_s\":\n",
    "            a.append(\"SC\")\n",
    "            b.append([1]*num_tr)        \n",
    "        elif i[1:4] == \"M_o\":\n",
    "            a.append(\"OM\")\n",
    "            b.append([2]*num_tr)\n",
    "        elif i[1:4] == \"C_o\":\n",
    "            a.append(\"OC\")\n",
    "            b.append([3]*num_tr)     \n",
    "        else:\n",
    "            a.append(\"Re\")\n",
    "            b.append([4]*num_tr)     \n",
    "    return a, b[1:]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "negative-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cond_index(sub_ses_labels):\n",
    "    \"\"\"\n",
    "    For the array of ordered run names (i.e.'Re', 'SM',) find the two indexes per condition\n",
    "    \"\"\" \n",
    "    lab_inx = []\n",
    "\n",
    "    a = []\n",
    "    b = []\n",
    "    c = []\n",
    "    d = []\n",
    "    e = []\n",
    "\n",
    "    for i in enumerate(sub_ses_labels):\n",
    "        if i[1] == \"SM\":\n",
    "            # append the index according to where it appeared in the array\n",
    "            a.append(i[0])\n",
    "        if i[1] == \"SC\":\n",
    "            b.append(i[0])\n",
    "        if i[1] == \"OM\":\n",
    "            c.append(i[0])\n",
    "        if i[1] == \"OC\":\n",
    "            d.append(i[0])\n",
    "\n",
    "    # Create a dictionary where each key contains the appropriate indexes\n",
    "    lab_indic = {\n",
    "        'SM' : a,\n",
    "        'SC' : b,\n",
    "        'OM' : c,\n",
    "        'OC' : d,\n",
    "        'RE' : [0,9]\n",
    "    }\n",
    "    return lab_indic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "packed-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_epi_data(sub, ses, task,run, space):\n",
    "  # Load MRI file\n",
    "    if space == \"MNI\":\n",
    "        epi_in = os.path.join(data_dir, sub, ses, 'func', \"%s_%s_task-%s_run-%s_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz\" % (sub, ses, task,run))\n",
    "    elif space == \"T1\":\n",
    "        epi_in = os.path.join(data_dir, sub, ses, 'func', \"%s_%s_task-%s_run-%s_space-T1w_desc-preproc_bold.nii.gz\" % (sub, ses, task,run))\n",
    "    else:\n",
    "        print(\"wrong load epi input. check this function\")\n",
    "    epi_data = nib.load(epi_in)\n",
    "    print(\"Loading data from %s\" % (epi_in))\n",
    "    return epi_data\n",
    "\n",
    "def load_roi_mask(ROI_name, space):\n",
    "    if space == \"MNI\":\n",
    "        maskdir = os.path.join(rois_dir)    \n",
    "        print(\"expected shape: 78, 93,65\")\n",
    "    elif space == \"T1\":\n",
    "        maskdir = os.path.join(rois_dir+ \"/T1\")\n",
    "        print(\"expected shape: 56, 72,53\")\n",
    "    else:\n",
    "        print(\"wrong mask input. check this function\")\n",
    "    # load the mask\n",
    "    maskfile = os.path.join(maskdir, \"%s.nii\" % (ROI_name))\n",
    "    mask = nib.load(maskfile)\n",
    "    print(\"mask shape: \", mask.shape)\n",
    "    print(\"Loaded %s mask\" % (ROI_name))\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "supported-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_mask(sub, num_runs,reg, ses=\"ses-01\",task=\"Attn\"):\n",
    "    # This is based off of 'load_data' function in template\n",
    "    # Loads all fMRI runs into a matrix #\n",
    "    \"\"\"\n",
    "    reg = T1 or MNI registration?\n",
    "    norm_type = by Space or by Time? \n",
    "    \"\"\"\n",
    "    yoz = []\n",
    "    print(\"Begin intersecting, yeehaw\")\n",
    "    for run in range(1, num_runs + 1):\n",
    "        # Load epi data \n",
    "        epi = load_epi_data(sub,ses,task,run,reg)\n",
    "        # Load ROI data\n",
    "        roi_samp = compute_epi_mask(epi)\n",
    "     #   print(roi_samp)\n",
    "        #nifti_masker = NiftiMasker(mask_img=roi_samp)\n",
    "        #maskedData = nifti_masker.fit_transform(epi)\n",
    "        yoz.append(roi_samp)\n",
    "    #print(concatenated_data)\n",
    "    epi_data = nil.masking.intersect_masks(yoz)\n",
    "    print(\"all done wit da intersextion (lol)\")\n",
    "\n",
    "    return epi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "realistic-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find indices of where each condition occured in 10 runs for each sub\n",
    "def fnd_indices(sub,behav_p):\n",
    "    behav = pd.read_csv(os.path.join(behav_p, '%s_behav_cleaned.csv') % (sub))\n",
    "    # Define the column in behav to be used for creating labels # \n",
    "    label = behav.iloc[:,1]\n",
    "    # Create an array of labels [1] AND the order in which runs occured [0]#\n",
    "    sub_ses_labels = label_lists(label, 200)\n",
    "    ## Find run sequence, extraction condition indexes from behav data ## \n",
    "    return find_cond_index(sub_ses_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "certified-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into dict\n",
    "def org_bdata_dic(unsort_bdata, run_indexes, cond_a, cond_b): \n",
    "    \"\"\"\n",
    "    organize two runs for concatenation\n",
    "    Two runs of cond_a, then two runs of cond_b, in a dik\n",
    "    \n",
    "    \"\"\"\n",
    "    bold_dict = {}\n",
    "    a = [unsort_bdata[run_indexes[cond_a][0]], unsort_bdata[run_indexes[cond_a][1]]]\n",
    "    b = [unsort_bdata[run_indexes[cond_b][0]], unsort_bdata[run_indexes[cond_b][1]]]\n",
    "    bold_dict[cond_a] = a\n",
    "    bold_dict[cond_b] = b\n",
    "    \n",
    "    print(\"concatenated\", cond_a, cond_b)\n",
    "    return bold_dict\n",
    "\n",
    "# organize into list\n",
    "def org_bdata_list(unsort_bdata, run_indexes, cond_a, cond_b): \n",
    "    \"\"\"\n",
    "    organize two runs for concatenation\n",
    "    Two runs of cond_a, then two runs of cond_b\n",
    "    \n",
    "    \"\"\"\n",
    "    bold_data = []\n",
    "    a = [unsort_bdata[run_indexes[cond_a][0]], unsort_bdata[run_indexes[cond_a][1]]]\n",
    "    b = [unsort_bdata[run_indexes[cond_b][0]], unsort_bdata[run_indexes[cond_b][1]]]\n",
    "    print(\"concatenated\", cond_a, cond_b)\n",
    "    return a + b\n",
    "\n",
    "def org_bdata(unsort_bdata, run_indexes, cond_a, cond_b): \n",
    "    \"\"\"\n",
    "    organize two runs for concatenation\n",
    "    Two runs of cond_a, then two runs of cond_b\n",
    "    \n",
    "    \"\"\"\n",
    "    bold_data = []\n",
    "    a = [unsort_bdata[run_indexes[cond_a][0]], unsort_bdata[run_indexes[cond_a][1]]]\n",
    "    b = [unsort_bdata[run_indexes[cond_b][0]], unsort_bdata[run_indexes[cond_b][1]]]\n",
    "    print(\"returning\", cond_a, cond_b)\n",
    "    return np.asarray(a), np.asarray(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "strong-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"_205_noproc.npy\"\n",
    "\n",
    "# load data into a dictionary\n",
    "def load_smsc_fmri_list(sub,behav_p,cond_a,cond_b,suffix,int_mask):\n",
    "    cats = list(np.load(load_fmri + sub + suffix, allow_pickle =True))\n",
    "    #cats = load_fMRI3d(sub, 10, \"MNI\", 'space', int_mask)\n",
    "    # Find run labels from behavioral data\n",
    "    lab_indic = fnd_indices(sub, behav_p)\n",
    "    # Organize and concatenate bold data\n",
    "    return org_bdata_list(cats, lab_indic, cond_a, cond_b)\n",
    "\n",
    "# Load data into dictionary\n",
    "def load_smsc_fmri_dic(sub,behav_p,cond_a,cond_b,suffix,int_mask):\n",
    "    cats = list(np.load(load_fmri + sub + suffix, allow_pickle =True))\n",
    "    #cats = load_fMRI3d(sub, 10, \"MNI\", 'space', int_mask)\n",
    "    # Find run labels from behavioral data\n",
    "    lab_indic = fnd_indices(sub, behav_p)\n",
    "    # Organize and concatenate bold data\n",
    "    return org_bdata_dic(cats, lab_indic, cond_a, cond_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-modem",
   "metadata": {},
   "source": [
    "# Rest stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acquired-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_event_list_rest(sub, bpress, cond, cond_alt, run_dic, base_onset,comp_onset_list,stim_dur):\n",
    "    \"\"\"\n",
    "    this function reads in a condition for each sub\n",
    "    and returns the corresponding b4 + after events\n",
    "    \n",
    "    sub: subject number\n",
    "    bpress: array of button press onset times\n",
    "    cond: which condition do you want to create event dataframe fore\n",
    "    \"\"\"\n",
    "    all_tims = []\n",
    "    events = {}\n",
    "    # Convert alternate bpress to array #\n",
    "    bpress_arr = np.asarray(bpress[sub][cond_alt])\n",
    "    # Select runs to include according to run_dic, append\n",
    "    # * include runs of rest \n",
    "    all_tims = all_tims + list(bpress_arr[run_dic[sub]['RE']])\n",
    "    # how much to shift from button press onset\n",
    "    \n",
    "    ## create fake bpress for missing data if only one run ## \n",
    "    if len(all_tims) <2 and len(run_dic[sub]['RE']) >1:\n",
    "        all_tims = list(all_tims[0], all_tims[0])\n",
    "        \n",
    "    return all_tims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "inside-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_2conds_runs_fmri(sub,behav_p,cond_a,cond_b, run_dic, suffix=\"_205_noproc.npy\"):\n",
    "    \"\"\"\n",
    "    read in target conditions + subject info\n",
    "    output: 2 runs of condition A, as they were presented, then two runs of condition B\n",
    "    \"\"\"\n",
    "    cats = list(np.load(load_fmri + sub + suffix, allow_pickle =True)) \n",
    "    # Find run labels from behavioral data\n",
    "    lab_indic = fnd_indices(sub, behav_p)\n",
    "    # Organize and concatenate bold data\n",
    "    a, b = org_bdata(cats, lab_indic, cond_a, cond_b)\n",
    "    # Get the indices of the runs to include #\n",
    "    return list(a[run_dic[sub][cond_a]]), list(b[run_dic[sub][cond_b]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lyric-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_1cond_runs_fmri(sub,behav_p,cond_a, cond_b, run_dic, targ_run, suffix=\"_205_noproc.npy\"):\n",
    "    \"\"\"\n",
    "    read in target conditions + subject info\n",
    "    output: 2 runs of condition A, as they were presented, then two runs of condition B\n",
    "    \"\"\"\n",
    "    cats = list(np.load(load_fmri + sub + suffix, allow_pickle =True))\n",
    "    # Find run labels from behavioral data\n",
    "    lab_indic = fnd_indices(sub, behav_p)\n",
    "    # Organize and concatenate bold data\n",
    "    a, b = org_bdata(cats, lab_indic, cond_a, cond_b)\n",
    "    # Get the indices of the runs to include #\n",
    "    return list(a[run_dic[sub][cond_a]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-istanbul",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "minute-plate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsignal clean, smooth to imporve acc\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Note to Isaac ##\n",
    "\"\"\"\n",
    "signal clean, smooth to imporve acc\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-papua",
   "metadata": {},
   "source": [
    "# Define Static VARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sunset-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "data_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/bids/derivatives/fmriprep/\"\n",
    "#rois_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/rois/\"\n",
    "rois_dir = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/rois/results_masks/masks_mvpa\"\n",
    "behav_p = '/jukebox/graziano/coolCatIsaac/ATM/data/behavioral'\n",
    "load_bpress = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/results/bpress_GLM/behav\"\n",
    "load_fmri = \"/jukebox/graziano/coolCatIsaac/ATM/data/work/results/bpress_GLM/\"\n",
    "confounds = '/jukebox/graziano/coolCatIsaac/ATM/data/bids/derivatives/fmriprep/afni-head_mot/'\n",
    "confounds_dir = '/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/censor_hm/'\n",
    "\n",
    "# load whole brain mask\n",
    "int_mask = nib.load('/jukebox/graziano/coolCatIsaac/ATM/data/work/workspace/load_fcma/mask_10r_n22-subs.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-registrar",
   "metadata": {},
   "source": [
    "# dynamic vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "designed-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sublist for rest ##\n",
    "\"\"\"\n",
    "no 5,8, 19, 26\n",
    "\"sub-012\" - index range issue -- definitely due to the NO selection of runs for fMRI data -- we\n",
    "select runs to be included for bpress data but not fMRI data -- need to edit\n",
    "\"\"\"\n",
    "sub_list = [\"sub-000\", \"sub-001\",\"sub-002\",\"sub-003\",\"sub-004\",\"sub-006\",\"sub-007\",\n",
    "            \"sub-009\",\"sub-010\",\"sub-011\",\"sub-013\",\"sub-014\",\"sub-015\", \"sub-016\",\"sub-017\", \n",
    "            \"sub-018\",\"sub-020\", \"sub-021\", \"sub-022\",\"sub-023\",\"sub-024\",\"sub-025\",\"sub-027\"]\n",
    "\n",
    "\n",
    "\n",
    "# Ignore this chunk\n",
    "base_onset = -6\n",
    "comp_onset_list = [-3,0,3]\n",
    "stim_dur = 3\n",
    "\n",
    "#\n",
    "\n",
    "# range of trs to extract\n",
    "tr_range = np.arange(-4,6)\n",
    "num_dpoints = len(tr_range)\n",
    "# conditions\n",
    "cond_a = 'SM'\n",
    "# ** changed\n",
    "cond_b = 'RE'\n",
    "\n",
    "hm_thresh = str(3)\n",
    "\n",
    "## cluster coordinates to be extracted \n",
    "# coords = [(51,-42,44)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-parcel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cellular-highlight",
   "metadata": {},
   "source": [
    "# bpress data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "extreme-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bpress behavioral data -- overlap removed ##\n",
    "bpress = dict(enumerate(np.load(os.path.join(load_bpress, \"n28_4_conds_ts_press_ovrlpREMOV.npy\"), \n",
    "                                allow_pickle=True).flatten(),1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "combined-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "### runs to exclude with head motion accounted for and missing bpress runs deleted\n",
    "run_dic = dict(enumerate(np.load(os.path.join(confounds_dir, \"n28_runs_2_include_removNoBpress_delHMruns_threshp%s.npy\") %(hm_thresh), \n",
    "                                allow_pickle=True).flatten(),1))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-drink",
   "metadata": {},
   "source": [
    "# Confounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "attempted-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_sub = dict(enumerate(np.load(os.path.join(confounds_dir, 'n28_conf+cens_MERGE_removNoBpress_delHMruns_threshp%s_glm.npy')%(hm_thresh), \n",
    "                                          allow_pickle = True).flatten(),1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "institutional-mileage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-000\n",
      "sub-001\n",
      "sub-002\n",
      "sub-003\n",
      "sub-004\n",
      "sub-006\n",
      "sub-007\n",
      "sub-009\n",
      "sub-010\n",
      "sub-011\n",
      "sub-013\n",
      "sub-014\n",
      "sub-015\n",
      "sub-016\n",
      "sub-017\n",
      "sub-018\n",
      "sub-020\n",
      "sub-021\n",
      "sub-022\n",
      "sub-023\n",
      "sub-024\n",
      "sub-025\n",
      "sub-027\n"
     ]
    }
   ],
   "source": [
    "## Create rest button presses ## \n",
    "for sub in sub_list:\n",
    "    print(sub)\n",
    "    temp = []\n",
    "    events_b = create_event_list_rest(sub, bpress, \"RE\",cond_a,run_dic,base_onset,comp_onset_list,stim_dur)\n",
    "    for runs in range(len(run_dic[sub]['RE'])):\n",
    "        temp.append(events_b[runs])   \n",
    "    bpress.setdefault(sub, {}).setdefault('RE',temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aquatic-threat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SM': [array([  7.8 ,  30.85,  53.28,  67.01, 116.3 , 144.7 , 162.57, 178.74,\n",
       "         200.63, 230.38, 247.79, 262.98, 290.36]),\n",
       "  array([  2.58,  21.65,  27.5 ,  47.7 ,  72.38, 107.04, 145.3 , 167.35,\n",
       "         193.33, 228.28, 261.19, 273.19])],\n",
       " 'SC': [array([ 42.98,  85.26, 121.66, 161.35, 196.7 , 228.55, 254.42, 285.95]),\n",
       "  array([ 40.25,  74.61, 102.55, 132.45, 164.88, 197.5 , 226.1 , 254.84,\n",
       "         288.92])],\n",
       " 'OM': [array([ 13.35,  39.75,  75.18,  85.  , 112.55, 155.96, 188.75, 200.98,\n",
       "         221.06, 238.51]),\n",
       "  array([ 34.31,  80.84, 101.98, 112.46, 153.61, 203.79, 234.54, 269.91,\n",
       "         287.09, 294.51])],\n",
       " 'OC': [array([ 18.3 ,  40.1 ,  62.01,  82.81, 104.95, 125.63, 148.23, 170.01,\n",
       "         193.55, 223.03, 244.19, 273.39, 304.86]),\n",
       "  array([ 23.57,  46.73,  70.63,  94.03, 117.08, 140.65, 165.68, 188.75,\n",
       "         216.85, 241.43, 264.7 , 292.83])],\n",
       " 'RE': [array([  7.8 ,  30.85,  53.28,  67.01, 116.3 , 144.7 , 162.57, 178.74,\n",
       "         200.63, 230.38, 247.79, 262.98, 290.36]),\n",
       "  array([  2.58,  21.65,  27.5 ,  47.7 ,  72.38, 107.04, 145.3 , 167.35,\n",
       "         193.33, 228.28, 261.19, 273.19])]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpress['sub-001']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-grade",
   "metadata": {},
   "source": [
    "# Load ROI mask - sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "reported-circuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected shape: 78, 93,65\n",
      "mask shape:  (78, 93, 65)\n",
      "Loaded l_prim_motor_sm_win2_mask mask\n"
     ]
    }
   ],
   "source": [
    "# sublist is set up top\n",
    "# sub_list = ['sub-001']\n",
    "\n",
    "# load in target ROI\n",
    "# roi = [\"l_prim_motor_sm_win2_mask\"]\n",
    "# roi = [\"dmPFC_ovlp_mask\"]\n",
    "# roi = [\"rTPJ_mask\"]\n",
    "\n",
    "roi = [\"l_prim_motor_sm_win2_mask\"]\n",
    "tr = 1.5\n",
    "high_pass = 1/128\n",
    "roi_mask = load_roi_mask(roi[0], \"MNI\")\n",
    "\n",
    "# REGRESS CONFOUNDS \n",
    "##** standardize = False ??? # \n",
    "masker = NiftiMasker(mask_img=roi_mask,smoothing_fwhm=2,\n",
    "                     standardize=True, detrend=True, high_pass=high_pass,\n",
    "                    t_r=tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-curve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "competent-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(sub_list, roi, roi_voxels, tr_range, cond_list):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    Inputs:\n",
    "        - sub_list: list of subs to iterate through\n",
    "        - roi: which roi to extract activations \n",
    "        - roi_voxels: number of voxels for the selected ROI\n",
    "        - tr_range: how many TRs should be averaged over?\n",
    "        - cond_list: Which two conditions are we comparing\n",
    "    Outputs:\n",
    "        - X: Activations for every button press across subs for a given window \n",
    "        - Y: array of integers describing if button press is cond a (0) or cond b (1)\n",
    "        - Groups: array of integers denoting button presses for each sub\n",
    "    \"\"\"\n",
    "    # load in target ROI\n",
    "    roi = roi\n",
    "    high_pass = 1/128\n",
    "    roi_mask = load_roi_mask(roi[0], \"MNI\")\n",
    "\n",
    "    # Create masker object and REGRESS CONFOUNDS \n",
    "    masker = NiftiMasker(mask_img=roi_mask,smoothing_fwhm=2,\n",
    "                         standardize=True, detrend=True, high_pass=high_pass,t_r=1.5)\n",
    "\n",
    "    ## X variable corresponds to a matrix that is: roi voxels x \n",
    "    X = np.empty((0, roi_voxels))\n",
    "    ## Set subject array ## \n",
    "    group_id = 0\n",
    "    groups = np.array([])\n",
    "    ## set array for labels ## \n",
    "    y = np.array([])\n",
    "\n",
    "    for sub in sub_list:\n",
    "        ## Which conditions? ## \n",
    "        cond_a = cond_list[0]\n",
    "        cond_b = cond_list[1]\n",
    "\n",
    "        print(sub)\n",
    "        # LOAD RUNS FOR COND A AND COND B\n",
    "        fmri_imgs = load_smsc_fmri_dic(sub,behav_p,cond_a,cond_b,suffix,int_mask)\n",
    "        # for each condition, extract patterns for each TR and average\n",
    "        for sing_cond in cond_list:\n",
    "            # for each run in available run dictionairy\n",
    "            for run in range(len(run_dic[sub][sing_cond])):\n",
    "                temp_run = []\n",
    "                print(\"condition: \", sing_cond)\n",
    "                #temp_pred = []\n",
    "                print(\"run\", run)\n",
    "\n",
    "                # Fit the masker object to extract a 2d matrix: voxels x TR [454, 205]\n",
    "                print(\"sub: \", sub)\n",
    "                roi_act = masker.fit_transform(fmri_imgs[sing_cond][run], confounds = conf_sub[sub][sing_cond][run])\n",
    "                print(roi_act.shape)\n",
    "\n",
    "\n",
    "                # extract BPRESS behavioral data # \n",
    "                linez = bpress[sub][sing_cond][run]\n",
    "                print(\"button press TRS:\", linez)\n",
    "\n",
    "                # Average activations \n",
    "                avg_activations = []\n",
    "                \n",
    "                # For each TR #\n",
    "                for tr in linez:\n",
    "                    # Find the tr that each onset occured - convert from seconds to TR\n",
    "                    print(tr)\n",
    "                    tr = round(tr/1.5)\n",
    "                    print(tr)\n",
    "                    temp = []\n",
    "                    for i in tr_range:\n",
    "                        try:\n",
    "                            # IF tr exists in the range of TRs add to temp array before averaging in the next step\n",
    "                            temp.append(roi_act[tr+i])\n",
    "                        except:\n",
    "                            continue\n",
    "                    print(\"temp length: \", len(temp))\n",
    "                    # If TRs exist for this button press, add it to the array # \n",
    "                    if (len(temp) > 0):\n",
    "                        X = np.vstack((X, np.mean(temp, axis=0)))\n",
    "                        print(\"X: \", X.shape)\n",
    "                        groups = np.append(groups, group_id)\n",
    "                        # note that I changed this to cond_a, cond_b\n",
    "                        if (sing_cond == cond_a):\n",
    "                            y = np.append(y, 0)\n",
    "\n",
    "                        elif (sing_cond == cond_b):\n",
    "                            y = np.append(y, 1)\n",
    "\n",
    "\n",
    "        group_id = group_id + 1\n",
    "    \n",
    "    return X, y, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "small-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_dict = {'motor': [\"l_prim_motor_sm_win2_mask\"], 'dmPFC': [\"dmPFC_ovlp_mask\"], 'rTPJ': [\"rTPJ_mask\"]}\n",
    "\n",
    "voxels_dict = {'motor': 454, 'dmPFC': 112, 'rTPJ': 187}\n",
    "\n",
    "tr_range_dict = {'1' : np.array([-3,-4]), '2' : np.array([-1,-2]), '3' : np.array([1,2]), \n",
    "                 '4' : np.array([3,4]), '5' : np.array([5,6]), '6' : np.array([7,8])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "incorporated-rainbow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected shape: 78, 93,65\n",
      "mask shape:  (78, 93, 65)\n",
      "Loaded dmPFC_ovlp_mask mask\n",
      "sub-000\n",
      "concatenated OM OC\n",
      "condition:  OM\n",
      "run 0\n",
      "sub:  sub-000\n",
      "(205, 112)\n",
      "button press TRS: [ 26.15  44.63  71.6   92.68 110.85 143.25 203.31 234.51 251.08 269.36\n",
      " 286.63 296.68]\n",
      "26.15\n",
      "17\n",
      "temp length:  2\n",
      "X:  (1, 112)\n",
      "44.63\n",
      "30\n",
      "temp length:  2\n",
      "X:  (2, 112)\n",
      "71.6\n",
      "48\n",
      "temp length:  2\n",
      "X:  (3, 112)\n",
      "92.68\n",
      "62\n",
      "temp length:  2\n",
      "X:  (4, 112)\n",
      "110.85\n",
      "74\n",
      "temp length:  2\n",
      "X:  (5, 112)\n",
      "143.25\n",
      "96\n",
      "temp length:  2\n",
      "X:  (6, 112)\n",
      "203.31\n",
      "136\n",
      "temp length:  2\n",
      "X:  (7, 112)\n",
      "234.51\n",
      "156\n",
      "temp length:  2\n",
      "X:  (8, 112)\n",
      "251.08\n",
      "167\n",
      "temp length:  2\n",
      "X:  (9, 112)\n",
      "269.36\n",
      "180\n",
      "temp length:  2\n",
      "X:  (10, 112)\n",
      "286.63\n",
      "191\n",
      "temp length:  2\n",
      "X:  (11, 112)\n",
      "296.68\n",
      "198\n",
      "temp length:  2\n",
      "X:  (12, 112)\n",
      "condition:  OC\n",
      "run 0\n",
      "sub:  sub-000\n",
      "(205, 112)\n",
      "button press TRS: [ 24.08  47.63  71.26  93.88 116.83 141.44 166.61 188.43 210.33 231.93\n",
      " 255.94 288.24]\n",
      "24.08\n",
      "16\n",
      "temp length:  2\n",
      "X:  (13, 112)\n",
      "47.63\n",
      "32\n",
      "temp length:  2\n",
      "X:  (14, 112)\n",
      "71.26\n",
      "48\n",
      "temp length:  2\n",
      "X:  (15, 112)\n",
      "93.88\n",
      "63\n",
      "temp length:  2\n",
      "X:  (16, 112)\n",
      "116.83\n",
      "78\n",
      "temp length:  2\n",
      "X:  (17, 112)\n",
      "141.44\n",
      "94\n",
      "temp length:  2\n",
      "X:  (18, 112)\n",
      "166.61\n",
      "111\n",
      "temp length:  2\n",
      "X:  (19, 112)\n",
      "188.43\n",
      "126\n",
      "temp length:  2\n",
      "X:  (20, 112)\n",
      "210.33\n",
      "140\n",
      "temp length:  2\n",
      "X:  (21, 112)\n",
      "231.93\n",
      "155\n",
      "temp length:  2\n",
      "X:  (22, 112)\n",
      "255.94\n",
      "171\n",
      "temp length:  2\n",
      "X:  (23, 112)\n",
      "288.24\n",
      "192\n",
      "temp length:  2\n",
      "X:  (24, 112)\n",
      "sub-001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e8e1a9226f18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mcond_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcond_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi_voxels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoxels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcond_list\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-574518052a8f>\u001b[0m in \u001b[0;36mget_activations\u001b[0;34m(sub_list, roi, roi_voxels, tr_range, cond_list)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# LOAD RUNS FOR COND A AND COND B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mfmri_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_smsc_fmri_dic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbehav_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcond_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcond_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;31m# for each condition, extract patterns for each TR and average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msing_cond\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcond_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-f3369e469875>\u001b[0m in \u001b[0;36mload_smsc_fmri_dic\u001b[0;34m(sub, behav_p, cond_a, cond_b, suffix, int_mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load data into dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_smsc_fmri_dic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbehav_p\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcond_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcond_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_fmri\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msub\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m#cats = load_fMRI3d(sub, 10, \"MNI\", 'space', int_mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Find run labels from behavioral data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 440\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mpickle_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;31m# Friendlier error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## ipython ## \n",
    "\"\"\"\n",
    "roi = 'dmPFC'\n",
    "window = '1'\n",
    "cond_a = 'OM'\n",
    "cond_b = 'OC'\n",
    "\n",
    "\"\"\"\n",
    "# slurm #\n",
    "roi = sys.argv[1]\n",
    "window = str(sys.argv[2])\n",
    "cond_a = sys.argv[3]\n",
    "cond_b = sys.argv[4]\n",
    "\n",
    "\n",
    "roi_vec = roi_dict[roi]\n",
    "voxels = voxels_dict[roi]\n",
    "tr_range = tr_range_dict[window]\n",
    "cond_list = [cond_a, cond_b]\n",
    "\n",
    "X, y, g = get_activations(sub_list, roi_vec, roi_voxels = voxels, tr_range = tr_range, cond_list = cond_list )\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(g.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-gardening",
   "metadata": {},
   "source": [
    "## Save "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "### be sure to update if i do another version of this script! uncomment np.save !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "latest-universe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-fd6acd5b7433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%s.npy'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"groups\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# analysis 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# name of current analysis\n",
    "analysis = '%s_%s_%s_win%s' %(cond_a, cond_b, roi, window)\n",
    "\n",
    "# directory # \n",
    "out_dir = '/jukebox/graziano/coolCatIsaac/ATM/code/analysis/bpress/mindy_mvpa/activations'\n",
    "date = '2023-05-11'\n",
    "dictionary_name = f'roi_activations_{date}'\n",
    "#load dictionary\n",
    "d = dict(enumerate(np.load(os.path.join(out_dir, '%s.npy') %(dictionary_name), allow_pickle=True).flatten(),1))[1]\n",
    "\n",
    "d2 = {\"X\": X, \"y\": y, \"groups\":g}\n",
    "# analysis 1\n",
    "d[analysis] = d2\n",
    "\n",
    "# save again\n",
    "#np.save(os.path.join(out_dir, '%s.npy') %(dictionary_name), d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "computational-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.load(os.path.join(out_dir, '%s.npy') %(dictionary_name), allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "declared-hamburg",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roi_activations_2023-05-11'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "maritime-dodge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([[-0.06401171, -0.13549575,  1.19781965, ...,  0.30585918,\n",
       "          0.39433611,  0.38492087],\n",
       "        [-1.23475914,  0.44703869, -1.18293423, ..., -0.08754643,\n",
       "          0.61671515,  0.23595011],\n",
       "        [ 0.60111049, -0.0744415 , -0.59334403, ...,  0.38838253,\n",
       "         -1.37739997,  0.16042628],\n",
       "        ...,\n",
       "        [-0.21130713, -1.32972936, -0.37508583, ..., -0.29618656,\n",
       "          0.18377865,  0.26413872],\n",
       "        [ 1.01127003,  0.55833063,  0.16742876, ..., -1.04826374,\n",
       "         -0.57662485, -1.28245722],\n",
       "        [ 0.52063821,  0.49488812,  0.03239257, ...,  0.5932196 ,\n",
       "          0.66989975,  1.39183228]]),\n",
       " 'y': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1.]),\n",
       " 'groups': array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "         2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "         2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "         2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "         3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "         3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "         3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "         4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "         4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "         4.,  4.,  4.,  4.,  4.,  4.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "         5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "         5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "         5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "         5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "         5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "         5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "         5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  6.,  6.,  6.,  6.,  6.,\n",
       "         6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "         6.,  6.,  6.,  6.,  6.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "         7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "         7.,  7.,  7.,  7.,  7.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "         8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "         8.,  8.,  8.,  8.,  8.,  8.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "         9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "         9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "        10., 10., 10., 10., 10., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "        11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 12., 12.,\n",
       "        12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
       "        12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 12.,\n",
       "        12., 12., 12., 12., 12., 12., 12., 12., 12., 12., 13., 13., 13.,\n",
       "        13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13.,\n",
       "        13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13., 13.,\n",
       "        13., 13., 13., 13., 13., 13., 13., 13., 14., 14., 14., 14., 14.,\n",
       "        14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14.,\n",
       "        14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14., 14.,\n",
       "        14., 14., 14., 14., 14., 14., 14., 15., 15., 15., 15., 15., 15.,\n",
       "        15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15.,\n",
       "        15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15., 15.,\n",
       "        15., 15., 15., 15., 15., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16.,\n",
       "        16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 16., 17., 17.,\n",
       "        17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
       "        17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
       "        17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17., 17.,\n",
       "        17., 17., 17., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18.,\n",
       "        18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18.,\n",
       "        18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18., 18.,\n",
       "        18., 18., 18., 18., 18., 18., 18., 18., 19., 19., 19., 19., 19.,\n",
       "        19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19.,\n",
       "        19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19.,\n",
       "        19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 19.,\n",
       "        19., 19., 19., 19., 19., 19., 19., 19., 19., 19., 20., 20., 20.,\n",
       "        20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "        20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20., 20.,\n",
       "        20., 20., 20., 20., 20., 21., 21., 21., 21., 21., 21., 21., 21.,\n",
       "        21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21.,\n",
       "        21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21., 21.,\n",
       "        21., 21., 21., 21., 21., 21., 21., 22., 22., 22., 22., 22., 22.,\n",
       "        22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22.,\n",
       "        22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22.,\n",
       "        22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22., 22.,\n",
       "        22.])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['OM_OC_motor_win6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-latin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
